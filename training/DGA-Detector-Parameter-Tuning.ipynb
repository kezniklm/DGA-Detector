{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b0a1b0",
   "metadata": {},
   "source": [
    "# Tuning of parameters for DGA-Detector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27319649",
   "metadata": {},
   "source": [
    "## Loading data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8327cd7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lex_name_len</th>\n",
       "      <th>lex_has_digit</th>\n",
       "      <th>lex_phishing_keyword_count</th>\n",
       "      <th>lex_consecutive_chars</th>\n",
       "      <th>lex_tld_len</th>\n",
       "      <th>lex_tld_abuse_score</th>\n",
       "      <th>lex_sld_len</th>\n",
       "      <th>lex_sld_norm_entropy</th>\n",
       "      <th>lex_sld_digit_count</th>\n",
       "      <th>lex_sld_digit_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>mod_jaccard_tri-grams_benign</th>\n",
       "      <th>mod_jaccard_penta-grams_benign</th>\n",
       "      <th>mod_jaccard_bi-grams_dga</th>\n",
       "      <th>mod_jaccard_tri-grams_dga</th>\n",
       "      <th>mod_jaccard_penta-grams_dga</th>\n",
       "      <th>lex_avg_part_len</th>\n",
       "      <th>lex_stdev_part_lens</th>\n",
       "      <th>lex_longest_part_len</th>\n",
       "      <th>lex_shortest_sub_len</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12</td>\n",
       "      <td>0.237949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>27</td>\n",
       "      <td>0.151053</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.6554</td>\n",
       "      <td>15</td>\n",
       "      <td>0.221549</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411765</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>8</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>26</td>\n",
       "      <td>0.165993</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>dga</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691813</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691814</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.343750</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>0.528321</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691815</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.375272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691816</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>691817</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>691818 rows Ã— 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lex_name_len  lex_has_digit  lex_phishing_keyword_count  \\\n",
       "0                 15              0                           0   \n",
       "1                 31              1                           0   \n",
       "2                 19              0                           0   \n",
       "3                 13              0                           1   \n",
       "4                 30              1                           0   \n",
       "...              ...            ...                         ...   \n",
       "691813            15              0                           0   \n",
       "691814            30              0                           0   \n",
       "691815             9              0                           0   \n",
       "691816             7              0                           0   \n",
       "691817            10              0                           0   \n",
       "\n",
       "        lex_consecutive_chars  lex_tld_len  lex_tld_abuse_score  lex_sld_len  \\\n",
       "0                           2            2               0.0000           12   \n",
       "1                           2            3               0.0081           27   \n",
       "2                           1            3               0.6554           15   \n",
       "3                           1            4               0.0043            8   \n",
       "4                           1            3               0.0081           26   \n",
       "...                       ...          ...                  ...          ...   \n",
       "691813                      2           10               0.0000            4   \n",
       "691814                      2            5               0.0000            8   \n",
       "691815                      2            2               0.0000            6   \n",
       "691816                      3            3               0.0000            3   \n",
       "691817                      3            6               0.0000            3   \n",
       "\n",
       "        lex_sld_norm_entropy  lex_sld_digit_count  lex_sld_digit_ratio  ...  \\\n",
       "0                   0.237949                  0.0             0.000000  ...   \n",
       "1                   0.151053                  9.0             0.333333  ...   \n",
       "2                   0.221549                  0.0             0.000000  ...   \n",
       "3                   0.375000                  0.0             0.000000  ...   \n",
       "4                   0.165993                  8.0             0.307692  ...   \n",
       "...                      ...                  ...                  ...  ...   \n",
       "691813              0.375000                  0.0             0.000000  ...   \n",
       "691814              0.343750                  0.0             0.000000  ...   \n",
       "691815              0.375272                  0.0             0.000000  ...   \n",
       "691816              0.000000                  0.0             0.000000  ...   \n",
       "691817              0.000000                  0.0             0.000000  ...   \n",
       "\n",
       "        mod_jaccard_tri-grams_benign  mod_jaccard_penta-grams_benign  \\\n",
       "0                           0.538462                        0.000000   \n",
       "1                           0.172414                        0.000000   \n",
       "2                           0.411765                        0.133333   \n",
       "3                           0.454545                        0.222222   \n",
       "4                           0.142857                        0.000000   \n",
       "...                              ...                             ...   \n",
       "691813                      0.615385                        0.454545   \n",
       "691814                      0.714286                        0.307692   \n",
       "691815                      0.571429                        0.000000   \n",
       "691816                      0.400000                        0.000000   \n",
       "691817                      0.625000                        0.833333   \n",
       "\n",
       "        mod_jaccard_bi-grams_dga  mod_jaccard_tri-grams_dga  \\\n",
       "0                       0.857143                   0.076923   \n",
       "1                       0.900000                   0.137931   \n",
       "2                       0.888889                   0.529412   \n",
       "3                       0.833333                   0.272727   \n",
       "4                       0.689655                   0.107143   \n",
       "...                          ...                        ...   \n",
       "691813                  0.714286                   0.076923   \n",
       "691814                  0.862069                   0.285714   \n",
       "691815                  0.750000                   0.142857   \n",
       "691816                  0.800000                   0.000000   \n",
       "691817                  0.750000                   0.125000   \n",
       "\n",
       "        mod_jaccard_penta-grams_dga  lex_avg_part_len  lex_stdev_part_lens  \\\n",
       "0                          0.000000          7.000000             0.500000   \n",
       "1                          0.074074         15.000000             0.500000   \n",
       "2                          0.000000          9.000000             0.500000   \n",
       "3                          0.111111          6.000000             0.500000   \n",
       "4                          0.000000         14.500000             0.500000   \n",
       "...                             ...               ...                  ...   \n",
       "691813                     0.000000          4.333333             0.528321   \n",
       "691814                     0.000000          9.333333             0.528321   \n",
       "691815                     0.000000          4.000000             0.500000   \n",
       "691816                     0.000000          3.000000             0.000000   \n",
       "691817                     0.000000          4.500000             0.500000   \n",
       "\n",
       "        lex_longest_part_len  lex_shortest_sub_len   label  \n",
       "0                         12                    12     dga  \n",
       "1                         27                    27     dga  \n",
       "2                         15                    15     dga  \n",
       "3                          8                     8     dga  \n",
       "4                         26                    26     dga  \n",
       "...                      ...                   ...     ...  \n",
       "691813                     7                     4  benign  \n",
       "691814                    15                    23  benign  \n",
       "691815                     6                     6  benign  \n",
       "691816                     3                     3  benign  \n",
       "691817                     6                     3  benign  \n",
       "\n",
       "[691818 rows x 49 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "dga = pq.read_table(\"floor/02-Preprocessed-data/DGA/01-DGA-Features.parquet\")\n",
    "benign = pq.read_table(\"floor/02-Preprocessed-data/Benign/00-Benign-Features.parquet\")\n",
    "\n",
    "# realign schemas (parquet files save in nonsense orders)\n",
    "benign = benign.cast(dga.schema)\n",
    "\n",
    "dga = dga.append_column(\"label\", pa.array([\"dga\"] * len(dga)))\n",
    "benign = benign.append_column(\"label\", pa.array([\"benign\"] * len(benign)))\n",
    "\n",
    "dga = dga.drop([\"domain_name\"])\n",
    "benign = benign.drop([\"domain_name\"])\n",
    "\n",
    "# concatentate tables\n",
    "data = pa.concat_tables([dga, benign])\n",
    "df = data.to_pandas()\n",
    "\n",
    "# Handle NaNs\n",
    "df.fillna(-1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334ce8fb",
   "metadata": {},
   "source": [
    "## Subsampling the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "563a8bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = 1.0 # 1.0 means no subsample\n",
    "\n",
    "if subsample < 1.0:\n",
    "    df = df.sample(frac=subsample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b579353e",
   "metadata": {},
   "source": [
    "## Supressing unwanted warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6169018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='sklearn.*')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='xgboost.*')\n",
    "warnings.filterwarnings('ignore', message=\"Series.__getitem__ treating keys as positions is deprecated.*\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f736af",
   "metadata": {},
   "source": [
    "# Train-Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3460ca64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_map = {\"benign\": 0, \"dga\": 1}\n",
    "\n",
    "labels = df[\"label\"].apply(lambda x: class_map[x])  # y vector\n",
    "features = df.drop(\"label\", axis=1).copy()  # X matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42, shuffle=True, stratify=labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33624380",
   "metadata": {},
   "source": [
    "## Custom tuning metric\n",
    "\n",
    "Allow for setting weights for **Precision, F1, FPR, logloss** and **Overfitting** (difference between train and test scores).\n",
    "\n",
    "- If you want to use it, the **scoring** should be set to **FETA_Score** on hyperparameter tuning.\n",
    "- Otherwise, you can set **scoring** to metric you want: **neg_log_loss**, **precision**, **f1**, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3165e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, log_loss\n",
    "\n",
    "def FETA_Score(estimator, X, y):\n",
    "    # Getting predictions\n",
    "    y_pred = estimator.predict(X)\n",
    "    y_pred_proba = estimator.predict_proba(X)\n",
    "    \n",
    "    # Calculating precision\n",
    "    precision = precision_score(y, y_pred)\n",
    "    \n",
    "    # Calculating recall\n",
    "    recall = recall_score(y, y_pred)\n",
    "    \n",
    "    # Calculating F1-score\n",
    "    f1 = f1_score(y, y_pred)\n",
    "    \n",
    "    # Calculating FPR\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    \n",
    "    # Calculating Log Loss\n",
    "    logloss = log_loss(y, y_pred_proba)\n",
    "    \n",
    "    # Estimating overfitting\n",
    "    train_score = estimator.score(X_train, y_train) \n",
    "    validation_score = estimator.score(X, y) \n",
    "    overfitting = train_score - validation_score\n",
    "    \n",
    "    # Assigning weights\n",
    "    w_precision = 0.1     # Precision\n",
    "    w_recall = 0.2        # Recall\n",
    "    w_f1 = 0.6            # F1\n",
    "    w_fpr = -0.0          # Negative because lower FPR is better\n",
    "    w_logloss = -0.0      # Negative because lower log loss is better\n",
    "    w_overfitting = -0.1  # Negative because lower overfitting is better\n",
    "        \n",
    "    # Combining metrics with weights\n",
    "    combined_score = (w_precision * precision + w_recall * recall + w_f1 * f1 + w_fpr * fpr + w_overfitting * overfitting)\n",
    "    \n",
    "    return combined_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6c7a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tuning - graphs\n",
    "def DisplayTuningGraph(scores):\n",
    "    from numpy import array\n",
    "    from numpy.ma import masked_array\n",
    "\n",
    "    col_names = ['mean_train_score', 'mean_test_score']\n",
    "    means_df = scores[col_names]\n",
    "    ax = means_df.plot(kind='line', grid=True)\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    plt.rcParams[\"figure.figsize\"] = [12, 12]\n",
    "    plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "    plt.rcParams[\"figure.dpi\"] = 300\n",
    "\n",
    "    max_ids = means_df.idxmax(axis=0)\n",
    "\n",
    "    for i in range(len(max_ids)):\n",
    "        for col_name in col_names:\n",
    "            value = means_df[col_name][max_ids[i]]\n",
    "            id = max_ids[i]\n",
    "\n",
    "            color = 'r' if max_ids[i] == max_ids['mean_test_score'] else 'grey'\n",
    "\n",
    "            ax.scatter([id], [value],\n",
    "                      marker='o',\n",
    "                      color=color,\n",
    "                      label='point',)\n",
    "\n",
    "            ax.annotate(str(round(value, 3))+\"_ID=\"+str(id),\n",
    "                        (id, value),\n",
    "                        xytext=(id+3, value))\n",
    "    print(scores)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a496",
   "metadata": {},
   "source": [
    "# Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9629807",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression model\n",
    "log_reg_model = LogisticRegression(random_state=7)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"C\": [100],  # Regularization parameter (default: 1.0)\n",
    "    \"penalty\": [\"l2\"],  # None, l1, l2 (default), elasticnet (both l1 and l2)\n",
    "    \"max_iter\": [100, 1000, 3000, 5000],  # Maximum number of iterations (default: 100)\n",
    "    \"solver\": [\"liblinear\"],  # lbfgs, liblinear, newton-cg, newton-cholesky, sag, saga}\n",
    "}\n",
    "\n",
    "# Stratified K-Fold cross-validator\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=log_reg_model,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b5d8d3",
   "metadata": {},
   "source": [
    "# SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707d63e7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n",
      "[CV] END .......C=58, gamma=0.09, kernel=rbf, shrinking=True; total time=19.8min\n",
      "[CV] END .......C=58, gamma=0.09, kernel=rbf, shrinking=True; total time=18.2min\n",
      "[CV] END .......C=58, gamma=0.09, kernel=rbf, shrinking=True; total time=17.8min\n",
      "[CV] END .....C=58, gamma=0.09, kernel=rbf, shrinking=False; total time=114.8min\n",
      "[CV] END .....C=58, gamma=0.09, kernel=rbf, shrinking=False; total time=112.2min\n",
      "[CV] END .....C=58, gamma=0.09, kernel=rbf, shrinking=False; total time=113.3min\n",
      "[CV] END ........C=58, gamma=0.1, kernel=rbf, shrinking=True; total time=19.2min\n",
      "[CV] END ........C=58, gamma=0.1, kernel=rbf, shrinking=True; total time=18.0min\n",
      "[CV] END ........C=58, gamma=0.1, kernel=rbf, shrinking=True; total time=18.2min\n",
      "[CV] END ......C=58, gamma=0.1, kernel=rbf, shrinking=False; total time=114.2min\n",
      "[CV] END ......C=58, gamma=0.1, kernel=rbf, shrinking=False; total time=113.5min\n",
      "[CV] END ......C=58, gamma=0.1, kernel=rbf, shrinking=False; total time=114.0min\n",
      "[CV] END .......C=58, gamma=0.11, kernel=rbf, shrinking=True; total time=18.6min\n",
      "[CV] END .......C=58, gamma=0.11, kernel=rbf, shrinking=True; total time=19.4min\n",
      "[CV] END .......C=58, gamma=0.11, kernel=rbf, shrinking=True; total time=18.6min\n",
      "[CV] END .....C=58, gamma=0.11, kernel=rbf, shrinking=False; total time=117.8min\n",
      "[CV] END .....C=58, gamma=0.11, kernel=rbf, shrinking=False; total time=115.7min\n",
      "[CV] END .....C=58, gamma=0.11, kernel=rbf, shrinking=False; total time=114.3min\n",
      "[CV] END .......C=59, gamma=0.09, kernel=rbf, shrinking=True; total time=18.2min\n",
      "[CV] END .......C=59, gamma=0.09, kernel=rbf, shrinking=True; total time=17.4min\n",
      "[CV] END .......C=59, gamma=0.09, kernel=rbf, shrinking=True; total time=17.7min\n",
      "[CV] END .....C=59, gamma=0.09, kernel=rbf, shrinking=False; total time=114.4min\n",
      "[CV] END .....C=59, gamma=0.09, kernel=rbf, shrinking=False; total time=114.8min\n",
      "[CV] END .....C=59, gamma=0.09, kernel=rbf, shrinking=False; total time=116.5min\n",
      "[CV] END ........C=59, gamma=0.1, kernel=rbf, shrinking=True; total time=18.3min\n",
      "[CV] END ........C=59, gamma=0.1, kernel=rbf, shrinking=True; total time=18.3min\n",
      "[CV] END ........C=59, gamma=0.1, kernel=rbf, shrinking=True; total time=18.3min\n",
      "[CV] END ......C=59, gamma=0.1, kernel=rbf, shrinking=False; total time=116.0min\n",
      "[CV] END ......C=59, gamma=0.1, kernel=rbf, shrinking=False; total time=116.8min\n",
      "[CV] END ......C=59, gamma=0.1, kernel=rbf, shrinking=False; total time=114.4min\n",
      "[CV] END .......C=59, gamma=0.11, kernel=rbf, shrinking=True; total time=20.0min\n",
      "[CV] END .......C=59, gamma=0.11, kernel=rbf, shrinking=True; total time=18.5min\n",
      "[CV] END .......C=59, gamma=0.11, kernel=rbf, shrinking=True; total time=18.6min\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_x_train = copy.deepcopy(X_train)\n",
    "svm_x_test = copy.deepcopy(X_test)\n",
    "\n",
    "svm_y_train = copy.deepcopy(y_train)\n",
    "svm_y_test = copy.deepcopy(y_test)\n",
    "\n",
    "# Fill NaNs and scale features\n",
    "svm_x_train = svm_x_train.fillna(0)\n",
    "scaler = MinMaxScaler()\n",
    "svm_x_train = scaler.fit_transform(svm_x_train)\n",
    "\n",
    "# Support Vector Machine model\n",
    "svm = SVC(random_state=7)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"C\": [58, 59, 60],  # Regularization parameter\n",
    "    \"gamma\": [0.09, 0.1, 0.11, 0.12],  # Kernel coefficient\n",
    "    \"kernel\": [\"rbf\"],  # ['rbf', 'linear', 'poly', 'sigmoid']\n",
    "}\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=svm,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(svm_x_train, svm_y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb573a9",
   "metadata": {},
   "source": [
    "# Decision Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "76a3eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Decision Tree base model\n",
    "dt = DecisionTreeClassifier(n_jobs=-1, random_state=7)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"max_depth\": [30, 50],  # Maximum depths or None for no limit\n",
    "    \"min_samples_split\": range(\n",
    "        5, 10\n",
    "    ),  # Broader range for minimum number of samples required to split an internal node\n",
    "    \"min_samples_leaf\": [\n",
    "        5,\n",
    "        8,\n",
    "        9,\n",
    "    ],  # Range for minimum number of samples required at a leaf node\n",
    "    \"max_leaf_nodes\": [500],  # Maximum number of leaf nodes or None for no limit\n",
    "}\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa6d98b",
   "metadata": {},
   "source": [
    "# Random Forest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3c7a6cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "[CV 1/3] END max_depth=10, n_estimators=50;, score=(train=0.844, test=0.831) total time=   6.8s\n",
      "[CV 2/3] END max_depth=10, n_estimators=50;, score=(train=0.846, test=0.835) total time=   6.8s\n",
      "[CV 3/3] END max_depth=10, n_estimators=50;, score=(train=0.847, test=0.829) total time=   6.8s\n",
      "[CV 1/3] END max_depth=10, n_estimators=70;, score=(train=0.845, test=0.831) total time=   9.4s\n",
      "[CV 2/3] END max_depth=10, n_estimators=70;, score=(train=0.845, test=0.833) total time=   9.5s\n",
      "[CV 3/3] END max_depth=10, n_estimators=70;, score=(train=0.846, test=0.829) total time=   9.4s\n",
      "[CV 1/3] END max_depth=10, n_estimators=143;, score=(train=0.846, test=0.832) total time=  19.2s\n",
      "[CV 2/3] END max_depth=10, n_estimators=143;, score=(train=0.846, test=0.834) total time=  19.6s\n",
      "[CV 3/3] END max_depth=10, n_estimators=143;, score=(train=0.847, test=0.828) total time=  19.3s\n",
      "[CV 1/3] END max_depth=10, n_estimators=200;, score=(train=0.846, test=0.833) total time=  27.5s\n",
      "[CV 2/3] END max_depth=10, n_estimators=200;, score=(train=0.846, test=0.835) total time=  27.3s\n",
      "[CV 3/3] END max_depth=10, n_estimators=200;, score=(train=0.848, test=0.828) total time=  27.0s\n",
      "[CV 1/3] END max_depth=14, n_estimators=50;, score=(train=0.877, test=0.834) total time=   8.3s\n",
      "[CV 2/3] END max_depth=14, n_estimators=50;, score=(train=0.876, test=0.836) total time=   8.3s\n",
      "[CV 3/3] END max_depth=14, n_estimators=50;, score=(train=0.878, test=0.830) total time=   8.4s\n",
      "[CV 1/3] END max_depth=14, n_estimators=70;, score=(train=0.878, test=0.834) total time=  11.5s\n",
      "[CV 2/3] END max_depth=14, n_estimators=70;, score=(train=0.876, test=0.835) total time=  11.4s\n",
      "[CV 3/3] END max_depth=14, n_estimators=70;, score=(train=0.879, test=0.832) total time=  11.6s\n",
      "[CV 1/3] END max_depth=14, n_estimators=143;, score=(train=0.878, test=0.835) total time=  23.3s\n",
      "[CV 2/3] END max_depth=14, n_estimators=143;, score=(train=0.877, test=0.837) total time=  23.2s\n",
      "[CV 3/3] END max_depth=14, n_estimators=143;, score=(train=0.880, test=0.832) total time=  23.6s\n",
      "[CV 1/3] END max_depth=14, n_estimators=200;, score=(train=0.877, test=0.835) total time=  32.6s\n",
      "[CV 2/3] END max_depth=14, n_estimators=200;, score=(train=0.877, test=0.837) total time=  32.5s\n",
      "[CV 3/3] END max_depth=14, n_estimators=200;, score=(train=0.880, test=0.832) total time=  32.9s\n",
      "[CV 1/3] END max_depth=17, n_estimators=50;, score=(train=0.891, test=0.831) total time=   8.7s\n",
      "[CV 2/3] END max_depth=17, n_estimators=50;, score=(train=0.891, test=0.832) total time=   8.7s\n",
      "[CV 3/3] END max_depth=17, n_estimators=50;, score=(train=0.892, test=0.829) total time=   9.1s\n",
      "[CV 1/3] END max_depth=17, n_estimators=70;, score=(train=0.892, test=0.831) total time=  13.4s\n",
      "[CV 2/3] END max_depth=17, n_estimators=70;, score=(train=0.892, test=0.833) total time=  13.4s\n",
      "[CV 3/3] END max_depth=17, n_estimators=70;, score=(train=0.893, test=0.828) total time=  13.3s\n",
      "[CV 1/3] END max_depth=17, n_estimators=143;, score=(train=0.893, test=0.831) total time=  27.3s\n",
      "[CV 2/3] END max_depth=17, n_estimators=143;, score=(train=0.893, test=0.833) total time=  27.1s\n",
      "[CV 3/3] END max_depth=17, n_estimators=143;, score=(train=0.893, test=0.829) total time=  27.1s\n",
      "[CV 1/3] END max_depth=17, n_estimators=200;, score=(train=0.892, test=0.831) total time=  38.0s\n",
      "[CV 2/3] END max_depth=17, n_estimators=200;, score=(train=0.892, test=0.833) total time=  37.7s\n",
      "[CV 3/3] END max_depth=17, n_estimators=200;, score=(train=0.893, test=0.828) total time=  37.7s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# Random Forest base model\n",
    "rf = RandomForestClassifier(random_state=7, n_jobs=-1)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    # The function to measure the quality of a split. \"entropy\" is used for the information gain.\n",
    "    \"criterion\": \"entropy\",\n",
    "\n",
    "    # Weights associated with classes. In this case, class 0 has weight 1, and class 1 has weight 5.\n",
    "    # This is used to address imbalances in the training data.\n",
    "    \"class_weight\": {0: 1, 1: 5},\n",
    "\n",
    "    # The number of trees in the forest. A list of possible values to try.\n",
    "    \"n_estimators\": [99, 100, 125, 150, 175, 200],\n",
    "\n",
    "    # The maximum depth of each tree. Deeper trees can learn more detailed data specifics, at the risk of overfitting.\n",
    "    \"max_depth\": [11, 14, 18, 19],\n",
    "\n",
    "    # The minimum number of samples required to split an internal node. Higher values prevent the model from learning overly specific patterns, thus reducing overfitting.\n",
    "    \"min_samples_split\": [2, 3, 9, 10, 13, 17],\n",
    "\n",
    "    # The minimum number of samples a node must have to be considered a leaf. Can help in controlling overfitting.\n",
    "    \"min_samples_leaf\": [1, 2, 3, 6, 9],\n",
    "\n",
    "    # The maximum number of leaf nodes per tree. 'None' means unlimited. Limiting this number can effectively reduce model complexity.\n",
    "    \"max_leaf_nodes\": [None, 150, 180, 190]\n",
    "}\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f483d0",
   "metadata": {},
   "source": [
    "# AdaBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b519cc1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "[CV 1/2] END n_estimators=200;, score=(train=0.505, test=0.463) total time= 1.1min\n",
      "[CV 2/2] END n_estimators=200;, score=(train=0.504, test=0.468) total time= 1.1min\n",
      "[CV 1/2] END n_estimators=400;, score=(train=0.504, test=0.467) total time= 2.3min\n",
      "[CV 2/2] END n_estimators=400;, score=(train=0.504, test=0.471) total time= 2.1min\n",
      "[CV 1/2] END n_estimators=650;, score=(train=0.504, test=0.470) total time= 4.0min\n",
      "[CV 2/2] END n_estimators=650;, score=(train=0.504, test=0.473) total time= 3.6min\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# AdaBoost base model\n",
    "ada = AdaBoostClassifier(random_state=7)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"base_estimator\": [\n",
    "        DecisionTreeClassifier(max_depth=depth) for depth in range(1, 3)\n",
    "    ],  # This parameter defines the type of model AdaBoost will use as the weak learner.\n",
    "    \"n_estimators\": [150, 200, 250],  # Number of weak learners to train iteratively\n",
    "    \"learning_rate\": [\n",
    "        0.4,\n",
    "        0.75,\n",
    "        1,\n",
    "    ],  # Learning rate shrinks the contribution of each classifier\n",
    "}\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ada,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eac2d15",
   "metadata": {},
   "source": [
    "# XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "292df97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 8 candidates, totalling 16 fits\n",
      "[CV 1/2] END scale_pos_weight=1;, score=(train=0.900, test=0.870) total time=  19.3s\n",
      "[CV 2/2] END scale_pos_weight=1;, score=(train=0.900, test=0.870) total time=  19.5s\n",
      "[CV 1/2] END scale_pos_weight=2;, score=(train=0.900, test=0.871) total time=  19.9s\n",
      "[CV 2/2] END scale_pos_weight=2;, score=(train=0.900, test=0.871) total time=  19.8s\n",
      "[CV 1/2] END scale_pos_weight=3;, score=(train=0.900, test=0.872) total time=  20.3s\n",
      "[CV 2/2] END scale_pos_weight=3;, score=(train=0.900, test=0.872) total time=  20.2s\n",
      "[CV 1/2] END scale_pos_weight=4;, score=(train=0.900, test=0.873) total time=  20.5s\n",
      "[CV 2/2] END scale_pos_weight=4;, score=(train=0.900, test=0.872) total time=  20.3s\n",
      "[CV 1/2] END scale_pos_weight=4.5;, score=(train=0.900, test=0.873) total time=  20.5s\n",
      "[CV 2/2] END scale_pos_weight=4.5;, score=(train=0.900, test=0.873) total time=  20.7s\n",
      "[CV 1/2] END scale_pos_weight=5;, score=(train=0.900, test=0.872) total time=  20.5s\n",
      "[CV 2/2] END scale_pos_weight=5;, score=(train=0.900, test=0.872) total time=  20.6s\n",
      "[CV 1/2] END scale_pos_weight=5.2;, score=(train=0.900, test=0.872) total time=  20.5s\n",
      "[CV 2/2] END scale_pos_weight=5.2;, score=(train=0.900, test=0.873) total time=  20.8s\n",
      "[CV 1/2] END scale_pos_weight=6.28;, score=(train=0.900, test=0.873) total time=  20.9s\n",
      "[CV 2/2] END scale_pos_weight=6.28;, score=(train=0.900, test=0.872) total time=  21.0s\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Assuming df is your DataFrame and class_map maps your classes\n",
    "labels = df[\"label\"].apply(lambda x: class_map[x])\n",
    "features = df.drop(\"label\", axis=1).copy()\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.3, random_state=42, shuffle=True, stratify=labels\n",
    ")\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": 9,\n",
    "    \"eta\": 0.15,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"tree_method\": \"gpu_hist\",\n",
    "    \"min_child_weight\": 2.0,\n",
    "    \"subsample\": 0.6,\n",
    "    \"sampling_method\": \"gradient_based\",\n",
    "    \"alpha\": 0,\n",
    "    \"gamma\": 0.1,\n",
    "    \"lambda\": 1.0,\n",
    "    \"max_delta_step\": 0,\n",
    "    \"grow_policy\": \"lossguide\",\n",
    "    \"max_bin\": 512,\n",
    "    \"n_estimators\": 550,\n",
    "    \"eval_metric\": [\"error\", \"logloss\", \"auc\"],\n",
    "    \"random_state\": 7,\n",
    "}\n",
    "\n",
    "# XGBoost base model\n",
    "xgb = XGBClassifier(**params)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"scale_pos_weight\": [1, 2, 3, 4, 4.5, 5, 5.2, 6.28],\n",
    "    # Add other parameters here if needed\n",
    "}\n",
    "\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    # scoring='f1',  # You can change this to another scoring method if needed\n",
    "    scoring=FETA_Score,  # Custom metric\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "grid_search.fit(\n",
    "    X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False\n",
    ")\n",
    "\n",
    "# Best estimator\n",
    "best_xgb = grid_search.best_estimator_\n",
    "\n",
    "# Results\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6b2b92",
   "metadata": {},
   "source": [
    "# LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bfe075cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021870 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/3] END max_depth=10, n_estimators=897;, score=(train=1.000, test=0.972) total time=  13.9s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021585 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=10, n_estimators=897;, score=(train=1.000, test=0.972) total time=  14.4s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022467 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/3] END max_depth=10, n_estimators=897;, score=(train=1.000, test=0.971) total time=  14.3s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022800 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/3] END max_depth=10, n_estimators=1350;, score=(train=1.000, test=0.972) total time=  21.5s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.027588 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=10, n_estimators=1350;, score=(train=1.000, test=0.973) total time=  21.1s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.020052 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/3] END max_depth=10, n_estimators=1350;, score=(train=1.000, test=0.972) total time=  21.3s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021483 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/3] END max_depth=10, n_estimators=1400;, score=(train=1.000, test=0.973) total time=  22.0s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022203 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=10, n_estimators=1400;, score=(train=1.000, test=0.973) total time=  22.1s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.113473 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/3] END max_depth=10, n_estimators=1400;, score=(train=1.000, test=0.972) total time=  21.8s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021513 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 1/3] END max_depth=10, n_estimators=1600;, score=(train=1.000, test=0.973) total time=  25.5s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022862 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END max_depth=10, n_estimators=1600;, score=(train=1.000, test=0.973) total time=  25.3s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[CV 3/3] END max_depth=10, n_estimators=1600;, score=(train=1.000, test=0.972) total time=  24.7s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021628 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[CV 1/3] END max_depth=11, n_estimators=897;, score=(train=1.000, test=0.972) total time=  13.5s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022812 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=11, n_estimators=897;, score=(train=1.000, test=0.973) total time=  14.7s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021763 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 3/3] END max_depth=11, n_estimators=897;, score=(train=1.000, test=0.971) total time=  13.9s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.066112 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[CV 1/3] END max_depth=11, n_estimators=1350;, score=(train=1.000, test=0.973) total time=  25.3s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021781 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=11, n_estimators=1350;, score=(train=1.000, test=0.974) total time=  21.1s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022875 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 3/3] END max_depth=11, n_estimators=1350;, score=(train=1.000, test=0.972) total time=  21.2s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.140323 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[CV 1/3] END max_depth=11, n_estimators=1400;, score=(train=1.000, test=0.973) total time=  26.6s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022061 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=11, n_estimators=1400;, score=(train=1.000, test=0.974) total time=  21.5s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022896 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 3/3] END max_depth=11, n_estimators=1400;, score=(train=1.000, test=0.972) total time=  21.9s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.021353 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12388\n",
      "[LightGBM] [Info] Number of data points in the train set: 233764, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136454 -> initscore=-1.845061\n",
      "[LightGBM] [Info] Start training from score -1.845061\n",
      "[CV 1/3] END max_depth=11, n_estimators=1600;, score=(train=1.000, test=0.973) total time=  24.6s\n",
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022208 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12365\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 2/3] END max_depth=11, n_estimators=1600;, score=(train=1.000, test=0.974) total time=  25.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 31898, number of negative: 201867\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022888 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 12400\n",
      "[LightGBM] [Info] Number of data points in the train set: 233765, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845066\n",
      "[LightGBM] [Info] Start training from score -1.845066\n",
      "[CV 3/3] END max_depth=11, n_estimators=1600;, score=(train=1.000, test=0.972) total time=  26.0s\n",
      "[LightGBM] [Info] Number of positive: 47847, number of negative: 302800\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.127560 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 12392\n",
      "[LightGBM] [Info] Number of data points in the train set: 350647, number of used features: 135\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.136453 -> initscore=-1.845064\n",
      "[LightGBM] [Info] Start training from score -1.845064\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "# LightGBM base model\n",
    "lgbm = LGBMClassifier(n_jobs=-1, random_state=7)\n",
    "\n",
    "# Grid of parameters to search\n",
    "grid = {\n",
    "    \"objective\": \"binary\",  # binary classification\n",
    "    \"boosting_type\": \"gbdt\",  # Gradient Boosting Decision Tree\n",
    "    \"min_child_samples\": 25,\n",
    "    \"colsample_bytree\": 1,\n",
    "    \"reg_lambda\": 0.45,\n",
    "    \"subsample\": 0.85,\n",
    "    \"subsample_freq\": 1,\n",
    "    \"subsample_for_bin\": 200000,\n",
    "    \"min_split_gain\": 0.01,\n",
    "    \"n_estimators\": [1050, 1150, 1200],\n",
    "    \"max_depth\": [11, 12, 13, 14],\n",
    "    \"num_leaves\": [28, 30, 32],\n",
    "    \"learning_rate\": [0.075, 0.1, 0.15],\n",
    "    \"scale_pos_weight\": [1.4, 1.5, 1.6, 1.7]\n",
    "}\n",
    "\n",
    "# Stratified K-Folds cross-validator\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=7)\n",
    "\n",
    "# Grid Search\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lgbm,\n",
    "    param_grid=grid,\n",
    "    cv=cv,\n",
    "    scoring=FETA_Score,\n",
    "    verbose=3,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,  # Use all available CPUs\n",
    ")\n",
    "\n",
    "# Fit GridSearchCV to the scaled training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results of the grid search in a DataFrame\n",
    "scores = pd.DataFrame(grid_search.cv_results_)\n",
    "\n",
    "# Display tuning graph of grid search results\n",
    "DisplayTuningGraph(scores)\n",
    "\n",
    "# Set Pandas to display up to 50 columns of the DataFrame\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "\n",
    "# Print 5 best scores based on rank\n",
    "scores.sort_values(\"rank_test_score\").head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
